{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we import our libriries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Football</td>\n",
       "      <td>Football is a family of team sports that invol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>History</td>\n",
       "      <td>History (from Greek ἱστορία, historia, meaning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Machine learning is an application of artifici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No Sql</td>\n",
       "      <td>A NoSQL (originally referring' to non-SQL' or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Elastic Search</td>\n",
       "      <td>Elasticsearch is a distributed, open source se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Geraphic Design</td>\n",
       "      <td>Graphic design is a craft where professionals ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Motion Greaphics</td>\n",
       "      <td>Motion graphics are pieces of animation or dig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Information Retrieval</td>\n",
       "      <td>Information retrieval (IR) is the activity of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Data science</td>\n",
       "      <td>Data science is an inter-disciplinary field th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Swimming is an individual or team racing sport...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Document Title                                            Content\n",
       "0               Football  Football is a family of team sports that invol...\n",
       "1                History  History (from Greek ἱστορία, historia, meaning...\n",
       "2       Machine Learning  Machine learning is an application of artifici...\n",
       "3                 No Sql  A NoSQL (originally referring' to non-SQL' or ...\n",
       "4         Elastic Search  Elasticsearch is a distributed, open source se...\n",
       "5        Geraphic Design  Graphic design is a craft where professionals ...\n",
       "6       Motion Greaphics  Motion graphics are pieces of animation or dig...\n",
       "7  Information Retrieval  Information retrieval (IR) is the activity of ...\n",
       "8           Data science  Data science is an inter-disciplinary field th...\n",
       "9               Swimming  Swimming is an individual or team racing sport..."
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['Football', \"Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal. Unqualified, the word football normally means the form of football that is the most popular where the word is used. Sports commonly called football include association football (known as soccer in some countries); gridiron football (specifically American football or Canadian football); Australian rules football; rugby football (either rugby union or rugby league); and Gaelic football.These various forms of football share to varying extent common origins and are known as football codes.\"],\n",
    "       ['History', \"History (from Greek ἱστορία, historia, meaning inquiry; knowledge acquired by investigation is the study of the past. Events occurring before the invention of writing systems are considered prehistory. History is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Historians place the past in context using historical sources such as written documents, oral accounts, ecological markers, and material objects including art and artifacts.\"],\n",
    "       ['Machine Learning', \"Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.\"],\n",
    "       ['No Sql', \"A NoSQL (originally referring' to non-SQL' or non-relational') database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name NoSQL was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also sometimes called 'Not only SQL' to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.\"],\n",
    "       ['Elastic Search', \"Elasticsearch is a distributed, open source search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. Elasticsearch is built on Apache Lucene and was first released in 2010 by Elasticsearch N.V. (now known as Elastic). Known for its simple REST APIs, distributed nature, speed, and scalability, Elasticsearch is the central component of the Elastic Stack, a set of open source tools for data ingestion, enrichment, storage, analysis, and visualization. Commonly referred to as the ELK Stack (after Elasticsearch, Logstash, and Kibana), the Elastic Stack now includes a rich collection of lightweight shipping agents known as Beats for sending data to Elasticsearch.\"],\n",
    "       ['Geraphic Design', \"Graphic design is a craft where professionals create visual content to communicate messages. By applying visual hierarchy and page layout techniques, designers use typography and pictures to meet users’ specific needs and focus on the logic of displaying elements in interactive designs, to optimize the user experience.\"],\n",
    "       ['Motion Greaphics', \"Motion graphics are pieces of animation or digital footage which create the illusion of motion or rotation, and are usually combined with audio for use in multimedia projects. Motion graphics are usually displayed via electronic media technology, but may also be displayed via manual powered technology (e.g. thaumatrope, phenakistoscope, stroboscope, zoetrope, praxinoscope, flip book). The term distinguishes static graphics from those with a transforming appearance over time, without over-specifying the form. While any form of experimental or abstract animation can be called motion graphics, the term typically more explicitly refers to the commercial application of animation and effects to video, film, TV, and interactive applications. Motion graphics are exceptional way to communicate with viewer, and it can add depth to the story. Also it can give us a message by music and effective copy together, they use it to create ads, television title sequence, explaining a concept, and share a product video that help to communicate their message.\"],\n",
    "       ['Information Retrieval', \"Information retrieval (IR) is the activity of obtaining information system resources that are relevant to an information need from a collection of those resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.\"],\n",
    "       ['Data science', \"Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data.Data science is a 'concept to unify statistics, data analysis and their related methods' in order to 'understand and analyze actual phenomena' with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, domain knowledge and information science. Turing award winner Jim Gray imagined data science as a 'fourth paradigm' of science (empirical, theoretical, computational and now data-driven) and asserted that 'everything about science is changing because of the impact of information technology' and the data deluge.\"],\n",
    "       ['Swimming', \"Swimming is an individual or team racing sport that requires the use of one's entire body to move through water. The sport takes place in pools or open water (e.g., in a sea or lake). Competitive swimming is one of the most popular Olympic sports, with varied distance events in butterfly, backstroke, breaststroke, freestyle, and individual medley. In addition to these individual events, four swimmers can take part in either a freestyle or medley relay. A medley relay consists of four swimmers who will each swim a different stroke, ordered as backstroke, breaststroke, butterfly and freestyle.Swimming each stroke requires a set of specific techniques; in competition, there are distinct regulations concerning the acceptable form for each individual stroke. There are also regulations on what types of swimsuits, caps, jewelry and injury tape that are allowed at competitions. Although it is possible for competitive swimmers to incur several injuries from the sport, such as tendinitis in the shoulders or knees, there are also multiple health benefits associated with the sport.\"]\n",
    "       ] \n",
    "df = pd.DataFrame(data, columns = ['Document Title', 'Content']) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read our files and put it in dataframe object to make it easy to deal with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'History (from Greek ἱστορία, historia, meaning inquiry; knowledge acquired by investigation is the study of the past. Events occurring before the invention of writing systems are considered prehistory. History is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Historians place the past in context using historical sources such as written documents, oral accounts, ecological markers, and material objects including art and artifacts.'"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1].Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "import string  \n",
    "#let's preprocess our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    input_str = text.lower()\n",
    "    return input_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ana okaaa w anaa ortigaaaaa'"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_lower(\"ANa okaaa W anaa ortigaaaAA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    return text.translate(table)\n",
    "\n",
    "    #no_punct=[words for words in text if words not in string.punctation]\n",
    "    #words_wo_punct=''.join(no_punct)\n",
    "    #return words_wo_punct\n",
    "\n",
    "#news['title_wo_punct']=news['title'].apply(lambda x: remove_punctuation(x))\n",
    "#news.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amasksdjsdsd'"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation('amask''s.$$&^.\",,\";djsdsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a list of tokenized and stemmed words of any text\n",
    "def get_tokenized_list(doc_text):\n",
    "    doc_text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", doc_text)\n",
    "    doc_text = re.sub('[()]', '', doc_text)\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    doc_text= doc_text.translate(table)\n",
    "\n",
    "    doc_text = doc_text.lower()\n",
    "    tokens = nltk.word_tokenize(doc_text)\n",
    "    return tokens\n",
    "\n",
    "# This function will performing stemming on tokenized words\n",
    "def word_stemmer(token_list):\n",
    "    \n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed = []\n",
    "    for words in token_list:\n",
    "        stemmed.append(ps.stem(words))\n",
    "    return stemmed\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords from tokenized word list\n",
    "def remove_stopwords(doc_text):\n",
    "    \n",
    "    cleaned_text = []\n",
    "    for words in doc_text:\n",
    "        if words not in stop_words:\n",
    "            cleaned_text.append(words)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before anything\n",
      "History (from Greek ἱστορία, historia, meaning inquiry; knowledge acquired by investigation is the study of the past. Events occurring before the invention of writing systems are considered prehistory. History is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Historians place the past in context using historical sources such as written documents, oral accounts, ecological markers, and material objects including art and artifacts.\n",
      "_______________________________\n",
      "WORD TOKENS:\n",
      "['history', 'from', 'greek', 'ἱστορία', 'historia', 'meaning', 'inquiry', 'knowledge', 'acquired', 'by', 'investigation', 'is', 'the', 'study', 'of', 'the', 'past', 'events', 'occurring', 'before', 'the', 'invention', 'of', 'writing', 'systems', 'are', 'considered', 'prehistory', 'history', 'is', 'an', 'umbrella', 'term', 'that', 'relates', 'to', 'past', 'events', 'as', 'well', 'as', 'the', 'memory', 'discovery', 'collection', 'organization', 'presentation', 'and', 'interpretation', 'of', 'information', 'about', 'these', 'events', 'historians', 'place', 'the', 'past', 'in', 'context', 'using', 'historical', 'sources', 'such', 'as', 'written', 'documents', 'oral', 'accounts', 'ecological', 'markers', 'and', 'material', 'objects', 'including', 'art', 'and', 'artifacts']\n",
      "\n",
      "AFTER REMOVING STOPWORDS:\n",
      "['history', 'greek', 'ἱστορία', 'historia', 'meaning', 'inquiry', 'knowledge', 'acquired', 'investigation', 'study', 'past', 'events', 'occurring', 'invention', 'writing', 'systems', 'considered', 'prehistory', 'history', 'umbrella', 'term', 'relates', 'past', 'events', 'well', 'memory', 'discovery', 'collection', 'organization', 'presentation', 'interpretation', 'information', 'events', 'historians', 'place', 'past', 'context', 'using', 'historical', 'sources', 'written', 'documents', 'oral', 'accounts', 'ecological', 'markers', 'material', 'objects', 'including', 'art', 'artifacts']\n",
      "\n",
      "AFTER PERFORMING THE WORD STEMMING::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['histori',\n",
       " 'greek',\n",
       " 'ἱστορία',\n",
       " 'historia',\n",
       " 'mean',\n",
       " 'inquiri',\n",
       " 'knowledg',\n",
       " 'acquir',\n",
       " 'investig',\n",
       " 'studi',\n",
       " 'past',\n",
       " 'event',\n",
       " 'occur',\n",
       " 'invent',\n",
       " 'write',\n",
       " 'system',\n",
       " 'consid',\n",
       " 'prehistori',\n",
       " 'histori',\n",
       " 'umbrella',\n",
       " 'term',\n",
       " 'relat',\n",
       " 'past',\n",
       " 'event',\n",
       " 'well',\n",
       " 'memori',\n",
       " 'discoveri',\n",
       " 'collect',\n",
       " 'organ',\n",
       " 'present',\n",
       " 'interpret',\n",
       " 'inform',\n",
       " 'event',\n",
       " 'historian',\n",
       " 'place',\n",
       " 'past',\n",
       " 'context',\n",
       " 'use',\n",
       " 'histor',\n",
       " 'sourc',\n",
       " 'written',\n",
       " 'document',\n",
       " 'oral',\n",
       " 'account',\n",
       " 'ecolog',\n",
       " 'marker',\n",
       " 'materi',\n",
       " 'object',\n",
       " 'includ',\n",
       " 'art',\n",
       " 'artifact']"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for single document\n",
    "tokens = get_tokenized_list(df.iloc[1].Content)\n",
    "tokens\n",
    "print(\"Before anything\")\n",
    "print(df.iloc[1].Content)\n",
    "print(\"_______________________________\")\n",
    "print(\"WORD TOKENS:\")\n",
    "print(tokens)\n",
    "doc_text = remove_stopwords(tokens)\n",
    "print(\"\\nAFTER REMOVING STOPWORDS:\")\n",
    "print(doc_text)\n",
    "print(\"\\nAFTER PERFORMING THE WORD STEMMING::\")\n",
    "doc_text = word_stemmer(doc_text)\n",
    "doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori greek ἱστορία historia mean inquiri knowledg acquir investig studi past event occur invent write system consid prehistori histori umbrella term relat past event well memori discoveri collect organ present interpret inform event historian place past context use histor sourc written document oral account ecolog marker materi object includ art artifact'"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ = ' '.join(doc_text)\n",
    "doc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['footbal famili team sport involv vari degre kick ball score goal unqualifi word footbal normal mean form footbal popular word use sport commonli call footbal includ associ footbal known soccer countri gridiron footbal specif american footbal canadian footbal australian rule footbal rugbi footbal either rugbi union rugbi leagu gaelic footballthes variou form footbal share vari extent common origin known footbal code',\n",
       " 'histori greek ἱστορία historia mean inquiri knowledg acquir investig studi past event occur invent write system consid prehistori histori umbrella term relat past event well memori discoveri collect organ present interpret inform event historian place past context use histor sourc written document oral account ecolog marker materi object includ art artifact',\n",
       " 'machin learn applic artifici intellig ai provid system abil automat learn improv experi without explicitli program machin learn focus develop comput program access data use learn',\n",
       " 'nosql origin refer nonsql nonrel databas provid mechan storag retriev data model mean tabular relat use relat databas databas exist sinc late 1960 name nosql coin earli 21st centuri trigger need web 20 compani nosql databas increasingli use big data realtim web applic nosql system also sometim call sql emphas may support sqllike queri languag sit alongsid sql databas polyglotpersist architectur',\n",
       " 'elasticsearch distribut open sourc search analyt engin type data includ textual numer geospati structur unstructur elasticsearch built apach lucen first releas elasticsearch nv known elast known simpl rest api distribut natur speed scalabl elasticsearch central compon elast stack set open sourc tool data ingest enrich storag analysi visual commonli refer elk stack elasticsearch logstash kibana elast stack includ rich collect lightweight ship agent known beat send data elasticsearch',\n",
       " 'graphic design craft profession creat visual content commun messag appli visual hierarchi page layout techniqu design use typographi pictur meet user ’ specif need focu logic display element interact design optim user experi',\n",
       " 'motion graphic piec anim digit footag creat illus motion rotat usual combin audio use multimedia project motion graphic usual display via electron media technolog may also display via manual power technolog eg thaumatrop phenakistoscop stroboscop zoetrop praxinoscop flip book term distinguish static graphic transform appear time without overspecifi form form experiment abstract anim call motion graphic term typic explicitli refer commerci applic anim effect video film tv interact applic motion graphic except way commun viewer add depth stori also give us messag music effect copi togeth use creat ad televis titl sequenc explain concept share product video help commun messag',\n",
       " 'inform retriev ir activ obtain inform system resourc relev inform need collect resourc search base fulltext contentbas index inform retriev scienc search inform document search document also search metadata describ data databas text imag soundsautom inform retriev system use reduc call inform overload ir system softwar system provid access book journal document store manag document web search engin visibl ir applic',\n",
       " 'data scienc interdisciplinari field use scientif method process algorithm system extract knowledg insight mani structur unstructur data data scienc relat data mine machin learn big datadata scienc concept unifi statist data analysi relat method order understand analyz actual phenomena data use techniqu theori drawn mani field within context mathemat statist comput scienc domain knowledg inform scienc ture award winner jim gray imagin data scienc fourth paradigm scienc empir theoret comput datadriven assert everyth scienc chang impact inform technolog data delug',\n",
       " 'swim individu team race sport requir use one entir bodi move water sport take place pool open water eg sea lake competit swim one popular olymp sport vari distanc event butterfli backstrok breaststrok freestyl individu medley addit individu event four swimmer take part either freestyl medley relay medley relay consist four swimmer swim differ stroke order backstrok breaststrok butterfli freestyleswim stroke requir set specif techniqu competit distinct regul concern accept form individu stroke also regul type swimsuit cap jewelri injuri tape allow competit although possibl competit swimmer incur sever injuri sport tendin shoulder knee also multipl health benefit associ sport']"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = []\n",
    "for doc in df.Content:\n",
    "    \n",
    "    tokens = get_tokenized_list(doc)\n",
    "    doc_text = remove_stopwords(tokens)\n",
    "    doc_text  = word_stemmer(doc_text)\n",
    "    doc_text = ' '.join(doc_text)\n",
    "    cleaned_corpus.append(doc_text)\n",
    "cleaned_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  1,  1,  2,  1,  2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  2,  1,\n",
       "        1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  3,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  3,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  2,  4,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  2,  1,  1,  2,\n",
       "        4,  1,  1,  1,  5,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  2,  3,  1,  2,  3,  1,  1,  6,  2,  2,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  3,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  3,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,\n",
       "        2,  2,  1,  1,  2,  2,  1,  1,  1,  2,  1,  5,  2,  2,  2,  2,  1,\n",
       "        5,  1,  3,  1,  1,  1,  1,  2,  1,  1,  1,  1,  2,  1,  1,  2,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  4,  1,  7,  4,  1,  1,  1,\n",
       "        1,  1,  3,  1,  1,  1,  5,  1,  1,  3,  1,  1,  2,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  1,\n",
       "        2,  2,  1,  1,  1,  2,  8,  1,  1,  1,  1,  1,  1,  1,  8,  1,  2,\n",
       "        1,  2,  1,  1,  1,  1,  2,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  5,  1,  1,  1,  1,  1,  1,  1,  2,  1,  2,  1,  1,\n",
       "        1,  1,  1,  1,  3,  4,  1,  2,  2,  1,  1,  1,  2,  2,  1,  1,  1,\n",
       "        4,  1,  1,  2,  2,  2,  2,  3,  1,  2,  3,  1,  2,  1,  1,  3,  1,\n",
       "        1,  2,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "tf_vector = count_vect.fit_transform(cleaned_corpus)\n",
    "tf_vector.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1960': 1, '20': 1, '21st': 1, 'abil': 1, 'abstract': 1, 'accept': 1, 'access': 2, 'account': 1, 'acquir': 1, 'activ': 1, 'actual': 1, 'ad': 1, 'add': 1, 'addit': 1, 'agent': 1, 'ai': 1, 'algorithm': 1, 'allow': 1, 'alongsid': 1, 'american': 1, 'analysi': 2, 'analyt': 1, 'analyz': 1, 'anim': 3, 'apach': 1, 'api': 1, 'appear': 1, 'appli': 1, 'applic': 5, 'architectur': 1, 'art': 1, 'artifact': 1, 'artifici': 1, 'assert': 1, 'associ': 2, 'audio': 1, 'australian': 1, 'automat': 1, 'award': 1, 'backstrok': 2, 'ball': 1, 'base': 1, 'beat': 1, 'benefit': 1, 'big': 2, 'bodi': 1, 'book': 2, 'breaststrok': 2, 'built': 1, 'butterfli': 2, 'canadian': 1, 'cap': 1, 'central': 1, 'centuri': 1, 'chang': 1, 'code': 1, 'coin': 1, 'collect': 3, 'combin': 1, 'commerci': 1, 'common': 1, 'commonli': 2, 'commun': 3, 'compani': 1, 'competit': 4, 'compon': 1, 'comput': 3, 'concept': 2, 'concern': 1, 'consid': 1, 'consist': 1, 'content': 1, 'contentbas': 1, 'context': 2, 'copi': 1, 'countri': 1, 'craft': 1, 'creat': 3, 'data': 15, 'databas': 6, 'datadata': 1, 'datadriven': 1, 'degre': 1, 'delug': 1, 'depth': 1, 'describ': 1, 'design': 3, 'develop': 1, 'differ': 1, 'digit': 1, 'discoveri': 1, 'display': 3, 'distanc': 1, 'distinct': 1, 'distinguish': 1, 'distribut': 2, 'document': 5, 'domain': 1, 'drawn': 1, 'earli': 1, 'ecolog': 1, 'effect': 2, 'elast': 3, 'elasticsearch': 6, 'electron': 1, 'element': 1, 'elk': 1, 'emphas': 1, 'empir': 1, 'engin': 2, 'enrich': 1, 'entir': 1, 'event': 5, 'everyth': 1, 'exist': 1, 'experi': 2, 'experiment': 1, 'explain': 1, 'explicitli': 2, 'extent': 1, 'extract': 1, 'famili': 1, 'field': 2, 'film': 1, 'flip': 1, 'focu': 1, 'focus': 1, 'footag': 1, 'footbal': 12, 'footballthes': 1, 'form': 5, 'fourth': 1, 'freestyl': 2, 'freestyleswim': 1, 'fulltext': 1, 'gaelic': 1, 'geospati': 1, 'goal': 1, 'graphic': 6, 'gray': 1, 'greek': 1, 'gridiron': 1, 'health': 1, 'help': 1, 'hierarchi': 1, 'histor': 1, 'histori': 2, 'historia': 1, 'historian': 1, 'illus': 1, 'imag': 1, 'imagin': 1, 'impact': 1, 'improv': 1, 'includ': 4, 'increasingli': 1, 'incur': 1, 'index': 1, 'individu': 4, 'inform': 10, 'ingest': 1, 'injuri': 2, 'inquiri': 1, 'insight': 1, 'intellig': 1, 'interact': 2, 'interdisciplinari': 1, 'interpret': 1, 'invent': 1, 'investig': 1, 'involv': 1, 'ir': 3, 'jewelri': 1, 'jim': 1, 'journal': 1, 'kibana': 1, 'kick': 1, 'knee': 1, 'knowledg': 3, 'known': 5, 'lake': 1, 'languag': 1, 'late': 1, 'layout': 1, 'leagu': 1, 'learn': 5, 'lightweight': 1, 'logic': 1, 'logstash': 1, 'lucen': 1, 'machin': 3, 'manag': 1, 'mani': 2, 'manual': 1, 'marker': 1, 'materi': 1, 'mathemat': 1, 'mean': 3, 'mechan': 1, 'media': 1, 'medley': 3, 'meet': 1, 'memori': 1, 'messag': 3, 'metadata': 1, 'method': 2, 'model': 1, 'motion': 5, 'multimedia': 1, 'multipl': 1, 'music': 1, 'natur': 1, 'need': 3, 'nonrel': 1, 'nonsql': 1, 'normal': 1, 'nosql': 4, 'numer': 1, 'nv': 1, 'object': 1, 'obtain': 1, 'occur': 1, 'olymp': 1, 'open': 3, 'optim': 1, 'oral': 1, 'order': 2, 'organ': 1, 'origin': 2, 'overload': 1, 'overspecifi': 1, 'page': 1, 'paradigm': 1, 'past': 3, 'phenakistoscop': 1, 'phenomena': 1, 'pictur': 1, 'piec': 1, 'place': 2, 'polyglotpersist': 1, 'pool': 1, 'popular': 2, 'possibl': 1, 'power': 1, 'praxinoscop': 1, 'prehistori': 1, 'present': 1, 'process': 1, 'product': 1, 'profession': 1, 'program': 2, 'project': 1, 'provid': 3, 'queri': 1, 'race': 1, 'realtim': 1, 'reduc': 1, 'refer': 3, 'regul': 2, 'relat': 5, 'relay': 2, 'releas': 1, 'relev': 1, 'requir': 2, 'resourc': 2, 'rest': 1, 'retriev': 4, 'rich': 1, 'rotat': 1, 'rugbi': 3, 'rule': 1, 'scalabl': 1, 'scienc': 9, 'scientif': 1, 'score': 1, 'sea': 1, 'search': 6, 'send': 1, 'sequenc': 1, 'set': 2, 'sever': 1, 'share': 2, 'ship': 1, 'shoulder': 1, 'simpl': 1, 'sinc': 1, 'sit': 1, 'soccer': 1, 'softwar': 1, 'sometim': 1, 'soundsautom': 1, 'sourc': 3, 'specif': 3, 'speed': 1, 'sport': 7, 'sql': 2, 'sqllike': 1, 'stack': 3, 'static': 1, 'statist': 2, 'storag': 2, 'store': 1, 'stori': 1, 'stroboscop': 1, 'stroke': 3, 'structur': 2, 'studi': 1, 'support': 1, 'swim': 3, 'swimmer': 3, 'swimsuit': 1, 'tabular': 1, 'tape': 1, 'team': 2, 'techniqu': 3, 'technolog': 3, 'televis': 1, 'tendin': 1, 'term': 3, 'text': 1, 'textual': 1, 'thaumatrop': 1, 'theoret': 1, 'theori': 1, 'time': 1, 'titl': 1, 'togeth': 1, 'tool': 1, 'transform': 1, 'trigger': 1, 'ture': 1, 'tv': 1, 'type': 2, 'typic': 1, 'typographi': 1, 'umbrella': 1, 'understand': 1, 'unifi': 1, 'union': 1, 'unqualifi': 1, 'unstructur': 2, 'use': 12, 'user': 2, 'usual': 2, 'vari': 3, 'variou': 1, 'video': 2, 'viewer': 1, 'visibl': 1, 'visual': 3, 'water': 2, 'way': 1, 'web': 3, 'winner': 1, 'word': 2, 'write': 1, 'written': 1, 'zoetrop': 1, 'ἱστορία': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# TF for documents\n",
    "documents = [\"The sky is blue and red and sea sea sea blue.\", \"The sun is bright also the sea sea sea sun is shiny.\",\"the is red blue and cyan.\"]\n",
    "count_vectorizer = CountVectorizer(analyzer = 'word',stop_words='english')\n",
    "document_term_matrix = count_vectorizer.fit_transform(cleaned_corpus)\n",
    "word_list = count_vectorizer.get_feature_names()   \n",
    "count_list = document_term_matrix.toarray().sum(axis=0) \n",
    "\n",
    "Tf = dict(zip(word_list,count_list))\n",
    "print(Tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "______________________________________\n",
      "design 1\n",
      "graphic 1\n"
     ]
    }
   ],
   "source": [
    "# TF for query\n",
    "#I enterd the same content in document 3 to find id the accuary of the similarity is working and the result should be 1 \n",
    "query = [\"this is graphic design\"]\n",
    "freq_term_matrix = count_vectorizer.transform(query)\n",
    "print (freq_term_matrix.todense())\n",
    "print(\"______________________________________\")\n",
    "word_list_query = count_vectorizer.get_feature_names()   \n",
    "count_list_query = freq_term_matrix.toarray().sum(axis=0)\n",
    "Tf_query =  dict(zip(word_list_query,count_list_query))\n",
    "\n",
    "for key , value in Tf_query.items():\n",
    "    if(value > 0 ):\n",
    "        print(key , value)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#[[0 1 1 1]\n",
    "#[0 2 1 0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 1.78845736 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.29928298 2.70474809 2.29928298 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.01160091 2.70474809 2.70474809\n",
      " 2.70474809 2.29928298 2.29928298 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.29928298 2.70474809 2.70474809 2.70474809 2.29928298\n",
      " 1.6061358  2.29928298 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.29928298 2.70474809 2.70474809 2.29928298 2.70474809\n",
      " 2.70474809 2.29928298 2.70474809 2.70474809 2.29928298 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.01160091 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.01160091 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.01160091 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298 2.29928298\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.01160091\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.01160091 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.29928298 2.70474809 2.70474809 2.29928298 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298 2.70474809\n",
      " 2.70474809 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.01160091 2.70474809 2.70474809 2.70474809 2.70474809 2.01160091\n",
      " 2.70474809 2.01160091 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.29928298 2.70474809 2.29928298\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.29928298 2.01160091 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.29928298 2.01160091 2.29928298 2.70474809 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298 1.09531018\n",
      " 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.29928298 2.70474809 2.70474809 2.29928298 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809]\n"
     ]
    }
   ],
   "source": [
    "#IDF for documents\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit_transform(document_term_matrix)\n",
    "idf =  tfidf.idf_\n",
    "print ( idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1960', '20', '21st', 'abil', 'abstract', 'accept', 'access', 'account', 'acquir', 'activ', 'actual', 'ad', 'add', 'addit', 'agent', 'ai', 'algorithm', 'allow', 'alongsid', 'also', 'although', 'american', 'analysi', 'analyt', 'analyz', 'anim', 'apach', 'api', 'appear', 'appli', 'applic', 'architectur', 'art', 'artifact', 'artifici', 'assert', 'associ', 'audio', 'australian', 'automat', 'award', 'backstrok', 'ball', 'base', 'beat', 'benefit', 'big', 'bodi', 'book', 'breaststrok', 'built', 'butterfli', 'call', 'canadian', 'cap', 'central', 'centuri', 'chang', 'code', 'coin', 'collect', 'combin', 'commerci', 'common', 'commonli', 'commun', 'compani', 'competit', 'compon', 'comput', 'concept', 'concern', 'consid', 'consist', 'content', 'contentbas', 'context', 'copi', 'countri', 'craft', 'creat', 'data', 'databas', 'datadata', 'datadriven', 'degre', 'delug', 'depth', 'describ', 'design', 'develop', 'differ', 'digit', 'discoveri', 'display', 'distanc', 'distinct', 'distinguish', 'distribut', 'document', 'domain', 'drawn', 'earli', 'ecolog', 'effect', 'eg', 'either', 'elast', 'elasticsearch', 'electron', 'element', 'elk', 'emphas', 'empir', 'engin', 'enrich', 'entir', 'event', 'everyth', 'except', 'exist', 'experi', 'experiment', 'explain', 'explicitli', 'extent', 'extract', 'famili', 'field', 'film', 'first', 'flip', 'focu', 'focus', 'footag', 'footbal', 'footballthes', 'form', 'four', 'fourth', 'freestyl', 'freestyleswim', 'fulltext', 'gaelic', 'geospati', 'give', 'goal', 'graphic', 'gray', 'greek', 'gridiron', 'health', 'help', 'hierarchi', 'histor', 'histori', 'historia', 'historian', 'illus', 'imag', 'imagin', 'impact', 'improv', 'includ', 'increasingli', 'incur', 'index', 'individu', 'inform', 'ingest', 'injuri', 'inquiri', 'insight', 'intellig', 'interact', 'interdisciplinari', 'interpret', 'invent', 'investig', 'involv', 'ir', 'jewelri', 'jim', 'journal', 'kibana', 'kick', 'knee', 'knowledg', 'known', 'lake', 'languag', 'late', 'layout', 'leagu', 'learn', 'lightweight', 'logic', 'logstash', 'lucen', 'machin', 'manag', 'mani', 'manual', 'marker', 'materi', 'mathemat', 'may', 'mean', 'mechan', 'media', 'medley', 'meet', 'memori', 'messag', 'metadata', 'method', 'mine', 'model', 'motion', 'move', 'multimedia', 'multipl', 'music', 'name', 'natur', 'need', 'nonrel', 'nonsql', 'normal', 'nosql', 'numer', 'nv', 'object', 'obtain', 'occur', 'olymp', 'one', 'open', 'optim', 'oral', 'order', 'organ', 'origin', 'overload', 'overspecifi', 'page', 'paradigm', 'part', 'past', 'phenakistoscop', 'phenomena', 'pictur', 'piec', 'place', 'polyglotpersist', 'pool', 'popular', 'possibl', 'power', 'praxinoscop', 'prehistori', 'present', 'process', 'product', 'profession', 'program', 'project', 'provid', 'queri', 'race', 'realtim', 'reduc', 'refer', 'regul', 'relat', 'relay', 'releas', 'relev', 'requir', 'resourc', 'rest', 'retriev', 'rich', 'rotat', 'rugbi', 'rule', 'scalabl', 'scienc', 'scientif', 'score', 'sea', 'search', 'send', 'sequenc', 'set', 'sever', 'share', 'ship', 'shoulder', 'simpl', 'sinc', 'sit', 'soccer', 'softwar', 'sometim', 'soundsautom', 'sourc', 'specif', 'speed', 'sport', 'sql', 'sqllike', 'stack', 'static', 'statist', 'storag', 'store', 'stori', 'stroboscop', 'stroke', 'structur', 'studi', 'support', 'swim', 'swimmer', 'swimsuit', 'system', 'tabular', 'take', 'tape', 'team', 'techniqu', 'technolog', 'televis', 'tendin', 'term', 'text', 'textual', 'thaumatrop', 'theoret', 'theori', 'time', 'titl', 'togeth', 'tool', 'transform', 'trigger', 'ture', 'tv', 'type', 'typic', 'typographi', 'umbrella', 'understand', 'unifi', 'union', 'unqualifi', 'unstructur', 'us', 'use', 'user', 'usual', 'vari', 'variou', 'via', 'video', 'viewer', 'visibl', 'visual', 'water', 'way', 'web', 'well', 'winner', 'within', 'without', 'word', 'write', 'written', 'zoetrop', 'ἱστορία']\n",
      "  (0, 376)\t0.14170376961841147\n",
      "  (0, 363)\t0.07085188480920573\n",
      "  (0, 362)\t0.12046115022899778\n",
      "  (0, 359)\t0.02869205857378808\n",
      "  (0, 356)\t0.07085188480920573\n",
      "  (0, 355)\t0.07085188480920573\n",
      "  (0, 330)\t0.06023057511449889\n",
      "  (0, 309)\t0.12046115022899778\n",
      "  (0, 307)\t0.05269463595716372\n",
      "  (0, 302)\t0.07085188480920573\n",
      "  (0, 296)\t0.06023057511449889\n",
      "  (0, 289)\t0.07085188480920573\n",
      "  (0, 285)\t0.07085188480920573\n",
      "  (0, 284)\t0.2125556544276172\n",
      "  (0, 256)\t0.06023057511449889\n",
      "  (0, 242)\t0.06023057511449889\n",
      "  (0, 228)\t0.07085188480920573\n",
      "  (0, 207)\t0.05269463595716372\n",
      "  (0, 193)\t0.07085188480920573\n",
      "  (0, 188)\t0.12046115022899778\n",
      "  (0, 185)\t0.07085188480920573\n",
      "  (0, 179)\t0.07085188480920573\n",
      "  (0, 163)\t0.05269463595716372\n",
      "  (0, 150)\t0.07085188480920573\n",
      "  (0, 146)\t0.07085188480920573\n",
      "  :\t:\n",
      "  (9, 140)\t0.14887345268090993\n",
      "  (9, 138)\t0.14887345268090993\n",
      "  (9, 137)\t0.05536078831093647\n",
      "  (9, 117)\t0.12655603585421205\n",
      "  (9, 116)\t0.07443672634045496\n",
      "  (9, 106)\t0.06327801792710602\n",
      "  (9, 105)\t0.06327801792710602\n",
      "  (9, 96)\t0.07443672634045496\n",
      "  (9, 95)\t0.07443672634045496\n",
      "  (9, 91)\t0.07443672634045496\n",
      "  (9, 73)\t0.07443672634045496\n",
      "  (9, 71)\t0.07443672634045496\n",
      "  (9, 67)\t0.29774690536181986\n",
      "  (9, 54)\t0.07443672634045496\n",
      "  (9, 51)\t0.14887345268090993\n",
      "  (9, 49)\t0.14887345268090993\n",
      "  (9, 47)\t0.07443672634045496\n",
      "  (9, 45)\t0.07443672634045496\n",
      "  (9, 41)\t0.14887345268090993\n",
      "  (9, 36)\t0.06327801792710602\n",
      "  (9, 20)\t0.07443672634045496\n",
      "  (9, 19)\t0.0984394158458084\n",
      "  (9, 17)\t0.07443672634045496\n",
      "  (9, 13)\t0.07443672634045496\n",
      "  (9, 5)\t0.07443672634045496\n",
      "(10, 381)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizerX = TfidfVectorizer()\n",
    "vectorizerX.fit(cleaned_corpus)\n",
    "doc_vector = vectorizerX.transform(cleaned_corpus)\n",
    "print(vectorizerX.get_feature_names())\n",
    "print(doc_vector)\n",
    "\n",
    "\n",
    "print(doc_vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf for the documents words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.DataFrame(doc_vector.toarray(), columns=vectorizerX.get_feature_names())\n",
    "#df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'data science'\n",
    "query = get_tokenized_list(query)\n",
    "query = remove_stopwords(query)\n",
    "q = []\n",
    "for w in word_stemmer(query):\n",
    "    \n",
    "    q.append(w)\n",
    "q = ' '.join(q)\n",
    "q\n",
    "query_vector = vectorizerX.transform([q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.06018362, 0.07523155, 0.09554576,\n",
       "       0.        , 0.        , 0.0994585 , 0.70665316, 0.        ])"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate cosine similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosineSimilarities = cosine_similarity(doc_vector,query_vector).flatten()\n",
    "cosineSimilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document1 0.0\n",
      "Document2 0.0\n",
      "Document3 0.060183618031820936\n",
      "Document4 0.07523154862589974\n",
      "Document5 0.09554575750619823\n",
      "Document6 0.0\n",
      "Document7 0.0\n",
      "Document8 0.09945850338154702\n",
      "Document9 0.7066531619565609\n",
      "Document10 0.0\n"
     ]
    }
   ],
   "source": [
    "for index , value in enumerate(cosineSimilarities) :\n",
    "    print(\"Document\" + str(index+1) +\" \"+ str(value) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Ir_Documents\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_header(data):\n",
    "    try:\n",
    "        ind = data.index('\\n\\n')\n",
    "        data = data[ind:]\n",
    "    except:\n",
    "        print(\"No Header\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return np.char.strip(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_characters(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return np.char.strip(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return np.char.strip(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def convert_numbers(data):\n",
    "    data = np.char.replace(data, \"0\", \" zero \")\n",
    "    data = np.char.replace(data, \"1\", \" one \")\n",
    "    data = np.char.replace(data, \"2\", \" two \")\n",
    "    data = np.char.replace(data, \"3\", \" three \")\n",
    "    data = np.char.replace(data, \"4\", \" four \")\n",
    "    data = np.char.replace(data, \"5\", \" five \")\n",
    "    data = np.char.replace(data, \"6\", \" six \")\n",
    "    data = np.char.replace(data, \"7\", \" seven \")\n",
    "    data = np.char.replace(data, \"8\", \" eight \")\n",
    "    data = np.char.replace(data, \"9\", \" nine \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, query):\n",
    "    if not query:\n",
    "        \n",
    "        data = remove_header(data)        \n",
    "        data = convert_lower_case(data)\n",
    "        data = convert_numbers(data)\n",
    "        data = remove_punctuation(data) #remove comma seperately\n",
    "        data = remove_apostrophe(data)\n",
    "        data = remove_single_characters(data)\n",
    "        data = stemming(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Postings¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal. Unqualified, the word football normally means the form of football that is the most popular where the word is used. Sports commonly called football include association football (known as soccer in some countries); gridiron football (specifically American football or Canadian football); Australian rules football; rugby football (either rugby union or rugby league); and Gaelic football.These various forms of football share to varying extent common origins and are known as football codes.\n",
      "__________________\n",
      "History (from Greek ἱστορία, historia, meaning inquiry; knowledge acquired by investigation is the study of the past. Events occurring before the invention of writing systems are considered prehistory. History is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Historians place the past in context using historical sources such as written documents, oral accounts, ecological markers, and material objects including art and artifacts.\n",
      "__________________\n",
      "Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.\n",
      "__________________\n",
      "A NoSQL (originally referring' to non-SQL' or non-relational') database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name NoSQL was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also sometimes called 'Not only SQL' to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.\n",
      "__________________\n",
      "Elasticsearch is a distributed, open source search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. Elasticsearch is built on Apache Lucene and was first released in 2010 by Elasticsearch N.V. (now known as Elastic). Known for its simple REST APIs, distributed nature, speed, and scalability, Elasticsearch is the central component of the Elastic Stack, a set of open source tools for data ingestion, enrichment, storage, analysis, and visualization. Commonly referred to as the ELK Stack (after Elasticsearch, Logstash, and Kibana), the Elastic Stack now includes a rich collection of lightweight shipping agents known as Beats for sending data to Elasticsearch.\n",
      "__________________\n",
      "Graphic design is a craft where professionals create visual content to communicate messages. By applying visual hierarchy and page layout techniques, designers use typography and pictures to meet users’ specific needs and focus on the logic of displaying elements in interactive designs, to optimize the user experience.\n",
      "__________________\n",
      "Motion graphics are pieces of animation or digital footage which create the illusion of motion or rotation, and are usually combined with audio for use in multimedia projects. Motion graphics are usually displayed via electronic media technology, but may also be displayed via manual powered technology (e.g. thaumatrope, phenakistoscope, stroboscope, zoetrope, praxinoscope, flip book). The term distinguishes static graphics from those with a transforming appearance over time, without over-specifying the form. While any form of experimental or abstract animation can be called motion graphics, the term typically more explicitly refers to the commercial application of animation and effects to video, film, TV, and interactive applications. Motion graphics are exceptional way to communicate with viewer, and it can add depth to the story. Also it can give us a message by music and effective copy together, they use it to create ads, television title sequence, explaining a concept, and share a product video that help to communicate their message.\n",
      "__________________\n",
      "Information retrieval (IR) is the activity of obtaining information system resources that are relevant to an information need from a collection of those resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.\n",
      "__________________\n",
      "Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data.Data science is a 'concept to unify statistics, data analysis and their related methods' in order to 'understand and analyze actual phenomena' with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, domain knowledge and information science. Turing award winner Jim Gray imagined data science as a 'fourth paradigm' of science (empirical, theoretical, computational and now data-driven) and asserted that 'everything about science is changing because of the impact of information technology' and the data deluge.\n",
      "__________________\n",
      "Swimming is an individual or team racing sport that requires the use of one's entire body to move through water. The sport takes place in pools or open water (e.g., in a sea or lake). Competitive swimming is one of the most popular Olympic sports, with varied distance events in butterfly, backstroke, breaststroke, freestyle, and individual medley. In addition to these individual events, four swimmers can take part in either a freestyle or medley relay. A medley relay consists of four swimmers who will each swim a different stroke, ordered as backstroke, breaststroke, butterfly and freestyle.Swimming each stroke requires a set of specific techniques; in competition, there are distinct regulations concerning the acceptable form for each individual stroke. There are also regulations on what types of swimsuits, caps, jewelry and injury tape that are allowed at competitions. Although it is possible for competitive swimmers to incur several injuries from the sport, such as tendinitis in the shoulders or knees, there are also multiple health benefits associated with the sport.\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "for docs in df.Content:\n",
    "    print(docs)\n",
    "    print(\"__________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Header\n",
      "0\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n"
     ]
    }
   ],
   "source": [
    "postings = pd.DataFrame()\n",
    "frequency = pd.DataFrame()\n",
    "doc = 0\n",
    "\n",
    "for docs in df.Content:\n",
    "    preprocessed_text = preprocess(docs, False)\n",
    "    if doc%100 == 0:\n",
    "        print(doc)\n",
    "\n",
    "    tokens = word_tokenize(str(preprocessed_text))\n",
    "    \n",
    "    pos = 0\n",
    "    for token in tokens:\n",
    "        if token in postings:\n",
    "            p = postings[token][0]            \n",
    "\n",
    "            k = [a[0] for a in p]\n",
    "            if doc in k:\n",
    "                for a in p:\n",
    "                    if a[0] == doc:\n",
    "                        a[1].add(pos)\n",
    "            else:\n",
    "                p.append([doc,{pos}])\n",
    "                frequency[token][0] += 1\n",
    "        else:\n",
    "            postings.insert(value=[[[doc, {pos}]]], loc=0, column=token)\n",
    "            frequency.insert(value=[1], loc=0, column=token)\n",
    "\n",
    "        pos += 1\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benefit</th>\n",
       "      <th>health</th>\n",
       "      <th>multipl</th>\n",
       "      <th>knee</th>\n",
       "      <th>shoulder</th>\n",
       "      <th>tendin</th>\n",
       "      <th>sever</th>\n",
       "      <th>incur</th>\n",
       "      <th>possibl</th>\n",
       "      <th>although</th>\n",
       "      <th>...</th>\n",
       "      <th>vari</th>\n",
       "      <th>to</th>\n",
       "      <th>involv</th>\n",
       "      <th>that</th>\n",
       "      <th>sport</th>\n",
       "      <th>team</th>\n",
       "      <th>of</th>\n",
       "      <th>famili</th>\n",
       "      <th>is</th>\n",
       "      <th>footbal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[[9, {159}]]</td>\n",
       "      <td>[[9, {158}]]</td>\n",
       "      <td>[[9, {157}]]</td>\n",
       "      <td>[[9, {153}]]</td>\n",
       "      <td>[[9, {151}]]</td>\n",
       "      <td>[[9, {148}]]</td>\n",
       "      <td>[[9, {141}]]</td>\n",
       "      <td>[[9, {140}]]</td>\n",
       "      <td>[[9, {135}]]</td>\n",
       "      <td>[[9, {132}]]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, {9, 78}], [9, {44}]]</td>\n",
       "      <td>[[0, {8, 77, 13}], [1, {35}], [2, {14}], [3, {...</td>\n",
       "      <td>[[0, {7}]]</td>\n",
       "      <td>[[0, {26, 6}], [1, {33}], [2, {9, 34}], [3, {1...</td>\n",
       "      <td>[[0, {36, 5}], [9, {163, 7, 42, 145, 21}]]</td>\n",
       "      <td>[[0, {4}], [9, {5}]]</td>\n",
       "      <td>[[0, {24, 74, 3}], [1, {49, 14, 22}], [2, {5, ...</td>\n",
       "      <td>[[0, {2}]]</td>\n",
       "      <td>[[0, {1, 34, 27}], [1, {11, 29}], [2, {2}], [3...</td>\n",
       "      <td>[[0, {0, 70, 39, 42, 75, 50, 19, 53, 86, 56, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        benefit        health       multipl          knee      shoulder  \\\n",
       "0  [[9, {159}]]  [[9, {158}]]  [[9, {157}]]  [[9, {153}]]  [[9, {151}]]   \n",
       "\n",
       "         tendin         sever         incur       possibl      although  ...  \\\n",
       "0  [[9, {148}]]  [[9, {141}]]  [[9, {140}]]  [[9, {135}]]  [[9, {132}]]  ...   \n",
       "\n",
       "                        vari  \\\n",
       "0  [[0, {9, 78}], [9, {44}]]   \n",
       "\n",
       "                                                  to      involv  \\\n",
       "0  [[0, {8, 77, 13}], [1, {35}], [2, {14}], [3, {...  [[0, {7}]]   \n",
       "\n",
       "                                                that  \\\n",
       "0  [[0, {26, 6}], [1, {33}], [2, {9, 34}], [3, {1...   \n",
       "\n",
       "                                        sport                  team  \\\n",
       "0  [[0, {36, 5}], [9, {163, 7, 42, 145, 21}]]  [[0, {4}], [9, {5}]]   \n",
       "\n",
       "                                                  of      famili  \\\n",
       "0  [[0, {24, 74, 3}], [1, {49, 14, 22}], [2, {5, ...  [[0, {2}]]   \n",
       "\n",
       "                                                  is  \\\n",
       "0  [[0, {1, 34, 27}], [1, {11, 29}], [2, {2}], [3...   \n",
       "\n",
       "                                             footbal  \n",
       "0  [[0, {0, 70, 39, 42, 75, 50, 19, 53, 86, 56, 2...  \n",
       "\n",
       "[1 rows x 434 columns]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benefit</th>\n",
       "      <th>health</th>\n",
       "      <th>multipl</th>\n",
       "      <th>knee</th>\n",
       "      <th>shoulder</th>\n",
       "      <th>tendin</th>\n",
       "      <th>sever</th>\n",
       "      <th>incur</th>\n",
       "      <th>possibl</th>\n",
       "      <th>although</th>\n",
       "      <th>...</th>\n",
       "      <th>vari</th>\n",
       "      <th>to</th>\n",
       "      <th>involv</th>\n",
       "      <th>that</th>\n",
       "      <th>sport</th>\n",
       "      <th>team</th>\n",
       "      <th>of</th>\n",
       "      <th>famili</th>\n",
       "      <th>is</th>\n",
       "      <th>footbal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   benefit  health  multipl  knee  shoulder  tendin  sever  incur  possibl  \\\n",
       "0        1       1        1     1         1       1      1      1        1   \n",
       "\n",
       "   although  ...  vari  to  involv  that  sport  team  of  famili  is  footbal  \n",
       "0         1  ...     2  10       1     8      2     2  10       1   9        1  \n",
       "\n",
       "[1 rows x 434 columns]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings.to_pickle(title + \"_positional_postings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings = pd.read_pickle(title + \"_positional_postings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency.to_pickle(title + \"_positional_frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = pd.read_pickle(title + \"_positional_frequency\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Query¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_postings(word):\n",
    "    preprocessed_word = str(preprocess(word, True))\n",
    "    print(preprocessed_word)\n",
    "    print(\"Frequency:\",frequency[preprocessed_word][0])\n",
    "    print(\"Postings List:\",postings[preprocessed_word][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positions(posting_values, doc):\n",
    "    for posting_value in posting_values:\n",
    "        if posting_value[0] == doc:\n",
    "            return posting_value[1]\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphic\n",
      "Frequency: 2\n",
      "Postings List: [[5, {0}], [6, {1, 105, 82, 57, 29}]]\n"
     ]
    }
   ],
   "source": [
    "get_word_postings(\"graphic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def gen_init_set_matchings(word):\n",
    "#    init = []\n",
    "#    word_postings = postings[word][0]\n",
    "#    for word_posting in word_postings:\n",
    "#        for positions in word_posting[1]:\n",
    "#            init.append((word_posting[0], positions))\n",
    "#    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def match_positional_index(init, b):\n",
    "    matched_docs = []\n",
    "    for p in init:\n",
    "        doc = p[0]\n",
    "        pos = p[1]\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for k in b:\n",
    "            pos = pos+1\n",
    "            k_pos = postings[k][0]\n",
    "            docs_list = [z[0] for z in k_pos]\n",
    "            if doc in docs_list:\n",
    "                doc_positions = get_positions(k_pos, doc)\n",
    "                if pos in doc_positions:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count += 1\n",
    "                    break\n",
    "\n",
    "            if count == len(b):\n",
    "                matched_docs.append(p[0])\n",
    "    return set(matched_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    processed_query = preprocess(query, True)\n",
    "    print(processed_query)\n",
    "\n",
    "    query_tokens = word_tokenize(str(processed_query))\n",
    "    print(query_tokens)\n",
    "    \n",
    "    if len(query_tokens)==1:\n",
    "        print(\"Total Document Mathces\", [a[0] for a in postings[query][0]])\n",
    "        return [a[0] for a in postings[query][0]]\n",
    "    \n",
    "    init_word = query_tokens[0]\n",
    "    init_matches = gen_init_set_matchings(init_word)\n",
    "\n",
    "    query_tokens.pop(0)\n",
    "    total_matched_docs = match_positional_index(init_matches, query_tokens)\n",
    "    print(\"Total Document Matches:\", total_matched_docs)\n",
    "    return total_matched_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_file(file):\n",
    "    out_file = open(paths[file], 'r', encoding='cp1250')\n",
    "    out_text = out_file.read()\n",
    "    print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"footbal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "footbal\n",
      "['footbal']\n",
      "Total Document Mathces [0]\n"
     ]
    }
   ],
   "source": [
    "lists = run_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
